{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EAeROGs7mjLa",
    "outputId": "e2ee866f-4eae-46e3-e9db-5ab9c4cce01e"
   },
   "outputs": [],
   "source": [
    "# rm -r valid\n",
    "# ! unzip ~/Symbols/train.zip -d ~/\n",
    "# ! unzip ~/Symbols/valid.zip -d ~/\n",
    "# ! unzip ~/Symbols/test.zip -d ~/\n",
    "# ls\n",
    "# Unzipping the corresponding files for use in the JupyterLab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WK45mAI0mmBM",
    "outputId": "f2676a8b-8009-4e48-839e-7a73c7542414"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification.ipynb  test.pickle  train.pickle  \u001b[0m\u001b[01;34mvalid\u001b[0m/\n",
      "\u001b[01;34mModels\u001b[0m/               \u001b[01;34mtrain\u001b[0m/       train.zip     valid.zip\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o1pZN-2oyg8S",
    "outputId": "b22655b5-8214-45ae-907f-d7ebbdb61fa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "TsKx_kzknLX6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.python.keras.layers import Dense, Flatten, GlobalAveragePooling2D\n",
    "from keras import optimizers\n",
    "from keras import applications\n",
    "from keras.models import Model\n",
    "from keras.callbacks import History\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "iBzUCYWEunMl"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet import preprocess_input\n",
    "# from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "# from tensorflow.keras.applications.densenet import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uY5jyCEKupCn",
    "outputId": "c2417a9e-d20b-4491-fc7d-2f8a8bf4493e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26925 images belonging to 60 classes.\n",
      "Found 11540 images belonging to 60 classes.\n"
     ]
    }
   ],
   "source": [
    "# loading the training data\n",
    "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
    "                                #    shear_range=0.2,\n",
    "                                  #  zoom_range=0.4,\n",
    "                                  #  rotation_range=30,\n",
    "                                   )\n",
    "\n",
    "valid_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "# Here the training files are accessed. Place here the directory of the train and validation files\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        '/home/jovyan/Symbols/train',\n",
    "        target_size=(45, 45),\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = valid_datagen.flow_from_directory(\n",
    "    '/home/jovyan/Symbols/valid',\n",
    "    shuffle=False,\n",
    "    target_size=(45, 45),\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "zRNEHQ-YurdM"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet152, ResNet50, InceptionV3, DenseNet121, DenseNet201\n",
    "# Importing the corresponding models above for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "cMb4IuhcutKt"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# create the base pre-trained model\n",
    "model.add(ResNet50(input_shape=(45, 45,3), weights='imagenet', include_top=False))\n",
    "# add a global spatial average pooling layer\n",
    "model.add(GlobalAveragePooling2D())\n",
    "# let's add a fully-connected layer\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "# and a logistic layer -- let's say we have 60 classes\n",
    "model.add(Dense(len(train_generator.class_indices), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "LGv69HmSuvR6"
   },
   "outputs": [],
   "source": [
    "# specify training loss function \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.0001),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U3beHCfNuwzh",
    "outputId": "05287c91-7227-4a9c-ee47-3dd8ee7072e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "841/842 [============================>.] - ETA: 0s - loss: 0.6737 - accuracy: 0.8310INFO:tensorflow:Assets written to: /home/jovyan/Symbols/Models/resnet50e30/assets\n",
      "842/842 [==============================] - 75s 90ms/step - loss: 0.6737 - accuracy: 0.8310 - val_loss: 6.0021 - val_accuracy: 0.6894 - lr: 1.0000e-04\n",
      "Epoch 2/30\n",
      "842/842 [==============================] - ETA: 0s - loss: 0.2060 - accuracy: 0.9424INFO:tensorflow:Assets written to: /home/jovyan/Symbols/Models/resnet50e30/assets\n",
      "842/842 [==============================] - 71s 85ms/step - loss: 0.2060 - accuracy: 0.9424 - val_loss: 3.1686 - val_accuracy: 0.8209 - lr: 1.0000e-04\n",
      "Epoch 3/30\n",
      "841/842 [============================>.] - ETA: 0s - loss: 0.1451 - accuracy: 0.9598INFO:tensorflow:Assets written to: /home/jovyan/Symbols/Models/resnet50e30/assets\n",
      "842/842 [==============================] - 75s 89ms/step - loss: 0.1450 - accuracy: 0.9598 - val_loss: 2.4374 - val_accuracy: 0.8873 - lr: 1.0000e-04\n",
      "Epoch 4/30\n",
      "841/842 [============================>.] - ETA: 0s - loss: 0.1079 - accuracy: 0.9690INFO:tensorflow:Assets written to: /home/jovyan/Symbols/Models/resnet50e30/assets\n",
      "842/842 [==============================] - 71s 84ms/step - loss: 0.1082 - accuracy: 0.9689 - val_loss: 1.2019 - val_accuracy: 0.9123 - lr: 1.0000e-04\n",
      "Epoch 5/30\n",
      "841/842 [============================>.] - ETA: 0s - loss: 0.0860 - accuracy: 0.9745INFO:tensorflow:Assets written to: /home/jovyan/Symbols/Models/resnet50e30/assets\n",
      "842/842 [==============================] - 67s 79ms/step - loss: 0.0859 - accuracy: 0.9745 - val_loss: 0.8149 - val_accuracy: 0.9282 - lr: 1.0000e-04\n",
      "Epoch 6/30\n",
      "842/842 [==============================] - ETA: 0s - loss: 0.0753 - accuracy: 0.9792INFO:tensorflow:Assets written to: /home/jovyan/Symbols/Models/resnet50e30/assets\n",
      "842/842 [==============================] - 76s 91ms/step - loss: 0.0753 - accuracy: 0.9792 - val_loss: 0.5149 - val_accuracy: 0.9410 - lr: 1.0000e-04\n",
      "Epoch 7/30\n",
      "842/842 [==============================] - 39s 47ms/step - loss: 0.0633 - accuracy: 0.9831 - val_loss: 1.5816 - val_accuracy: 0.9139 - lr: 1.0000e-04\n",
      "Epoch 8/30\n",
      "842/842 [==============================] - 38s 45ms/step - loss: 0.0547 - accuracy: 0.9834 - val_loss: 1.4165 - val_accuracy: 0.9225 - lr: 1.0000e-04\n",
      "Epoch 9/30\n",
      "842/842 [==============================] - 37s 44ms/step - loss: 0.0439 - accuracy: 0.9878 - val_loss: 0.7382 - val_accuracy: 0.9409 - lr: 1.0000e-04\n",
      "Epoch 10/30\n",
      "842/842 [==============================] - 37s 44ms/step - loss: 0.0451 - accuracy: 0.9877 - val_loss: 1.1321 - val_accuracy: 0.9295 - lr: 1.0000e-04\n",
      "Epoch 11/30\n",
      "842/842 [==============================] - 37s 44ms/step - loss: 0.0446 - accuracy: 0.9893 - val_loss: 0.5456 - val_accuracy: 0.9452 - lr: 1.0000e-04\n",
      "Epoch 12/30\n",
      "842/842 [==============================] - 36s 43ms/step - loss: 0.0366 - accuracy: 0.9894 - val_loss: 1.1196 - val_accuracy: 0.9306 - lr: 1.0000e-04\n",
      "Epoch 13/30\n",
      "842/842 [==============================] - 37s 44ms/step - loss: 0.0338 - accuracy: 0.9915 - val_loss: 0.7348 - val_accuracy: 0.9366 - lr: 1.0000e-04\n",
      "Epoch 14/30\n",
      "842/842 [==============================] - 36s 43ms/step - loss: 0.0334 - accuracy: 0.9916 - val_loss: 0.5589 - val_accuracy: 0.9437 - lr: 1.0000e-04\n",
      "Epoch 15/30\n",
      "842/842 [==============================] - 37s 44ms/step - loss: 0.0312 - accuracy: 0.9926 - val_loss: 0.6415 - val_accuracy: 0.9393 - lr: 1.0000e-04\n",
      "Epoch 16/30\n",
      "842/842 [==============================] - 37s 44ms/step - loss: 0.0293 - accuracy: 0.9921 - val_loss: 0.7207 - val_accuracy: 0.9443 - lr: 1.0000e-04\n",
      "Epoch 17/30\n",
      "842/842 [==============================] - 37s 43ms/step - loss: 0.0248 - accuracy: 0.9942 - val_loss: 0.8721 - val_accuracy: 0.9386 - lr: 1.0000e-04\n",
      "Epoch 18/30\n",
      "842/842 [==============================] - 37s 44ms/step - loss: 0.0234 - accuracy: 0.9933 - val_loss: 0.7131 - val_accuracy: 0.9422 - lr: 1.0000e-04\n",
      "Epoch 19/30\n",
      "842/842 [==============================] - 37s 44ms/step - loss: 0.0225 - accuracy: 0.9943 - val_loss: 0.7300 - val_accuracy: 0.9481 - lr: 1.0000e-04\n",
      "Epoch 20/30\n",
      "842/842 [==============================] - 37s 44ms/step - loss: 0.0239 - accuracy: 0.9948 - val_loss: 0.7101 - val_accuracy: 0.9471 - lr: 1.0000e-04\n",
      "Epoch 21/30\n",
      "842/842 [==============================] - 37s 44ms/step - loss: 0.0182 - accuracy: 0.9954 - val_loss: 1.1532 - val_accuracy: 0.9309 - lr: 1.0000e-04\n",
      "Epoch 22/30\n",
      "842/842 [==============================] - 38s 45ms/step - loss: 0.0229 - accuracy: 0.9949 - val_loss: 1.7712 - val_accuracy: 0.9208 - lr: 1.0000e-04\n",
      "Epoch 23/30\n",
      "842/842 [==============================] - 38s 45ms/step - loss: 0.0201 - accuracy: 0.9949 - val_loss: 0.8184 - val_accuracy: 0.9366 - lr: 1.0000e-04\n",
      "Epoch 24/30\n",
      "841/842 [============================>.] - ETA: 0s - loss: 0.0208 - accuracy: 0.9954\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "842/842 [==============================] - 38s 45ms/step - loss: 0.0208 - accuracy: 0.9954 - val_loss: 2.5887 - val_accuracy: 0.9067 - lr: 1.0000e-04\n",
      "Epoch 25/30\n",
      "842/842 [==============================] - 37s 44ms/step - loss: 0.0067 - accuracy: 0.9981 - val_loss: 0.7869 - val_accuracy: 0.9451 - lr: 5.0000e-05\n",
      "Epoch 26/30\n",
      "842/842 [==============================] - 37s 44ms/step - loss: 0.0054 - accuracy: 0.9987 - val_loss: 0.9388 - val_accuracy: 0.9419 - lr: 5.0000e-05\n",
      "Epoch 27/30\n",
      "842/842 [==============================] - 38s 45ms/step - loss: 0.0043 - accuracy: 0.9990 - val_loss: 0.8335 - val_accuracy: 0.9482 - lr: 5.0000e-05\n",
      "Epoch 28/30\n",
      "842/842 [==============================] - 37s 44ms/step - loss: 0.0041 - accuracy: 0.9990 - val_loss: 1.0090 - val_accuracy: 0.9464 - lr: 5.0000e-05\n",
      "Epoch 29/30\n",
      "842/842 [==============================] - 37s 44ms/step - loss: 0.0031 - accuracy: 0.9991 - val_loss: 0.8779 - val_accuracy: 0.9497 - lr: 5.0000e-05\n",
      "Epoch 30/30\n",
      "842/842 [==============================] - 36s 42ms/step - loss: 0.0034 - accuracy: 0.9990 - val_loss: 0.9493 - val_accuracy: 0.9483 - lr: 5.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f312f069a58>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=3, verbose = 1)\n",
    "\n",
    "# Specify the name of the filepath where the model is to be saved. Here it is called resnet50e30 as it uses a pretrained ResNet50 under 30 epochs\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='/home/jovyan/Symbols/Models/resnet50e30',\n",
    "    monitor='val_loss',\n",
    "    save_weights_only=False,\n",
    "    save_best_only=True)\n",
    "\n",
    "model.fit(x=train_generator, validation_data=validation_generator, epochs = 30, callbacks=[reduce_lr,model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oEvlVXtYu0r2",
    "outputId": "8ced1d5b-55e0-4f48-daa4-ab69c413cf31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5082 images belonging to 61 classes.\n"
     ]
    }
   ],
   "source": [
    "# Here the model may be evaluated. First, import the test files\n",
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    '/home/jovyan/Symbols/test',\n",
    "    shuffle=False,\n",
    "    target_size=(45, 45),\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "dzRCvt1Wu2GR"
   },
   "outputs": [],
   "source": [
    "# test_generator.classes\n",
    "# test_generator.class_indices\n",
    "# test_generator.filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "D9yeC9zau6gg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1123573396300669 0.15072805981896892 0.1721763085399449 0.18929555293191658 0.2028728846910665\n"
     ]
    }
   ],
   "source": [
    "# The models top5 accuracies can be found below\n",
    "top1 = 0\n",
    "top2 = 0\n",
    "top3 = 0\n",
    "top4 = 0\n",
    "top5 = 0\n",
    "predict = model.predict(test_generator)\n",
    "\n",
    "for i in range(len(predict)):\n",
    "  result = np.argsort(-predict[i])\n",
    "  truth = test_generator.classes[i]\n",
    "  if truth in result[:5]: top5 += 1\n",
    "  if truth in result[:4]: top4 += 1\n",
    "  if truth in result[:3]: top3 += 1\n",
    "  if truth in result[:2]: top2 += 1\n",
    "  if truth in result[:1]: top1 += 1\n",
    "\n",
    "top1 = top1/len(predict)\n",
    "top2 = top2/len(predict)\n",
    "top3 = top3/len(predict)\n",
    "top4 = top4/len(predict)\n",
    "top5 = top5/len(predict)\n",
    "print(top1, top2, top3, top4, top5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Vs-pITaoo8gs"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "model = keras.models.load_model('/home/jovyan/Symbols/Models/resnet50e30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "iBzUCYWEunMl"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet import preprocess_input\n",
    "# from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "# from tensorflow.keras.applications.densenet import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uY5jyCEKupCn",
    "outputId": "c2417a9e-d20b-4491-fc7d-2f8a8bf4493e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26925 images belonging to 60 classes.\n",
      "Found 11540 images belonging to 60 classes.\n"
     ]
    }
   ],
   "source": [
    "# loading training data\n",
    "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
    "                                #    shear_range=0.2,\n",
    "                                  #  zoom_range=0.4,\n",
    "                                  #  rotation_range=30,\n",
    "                                   )\n",
    "\n",
    "valid_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "# Specify the directory of the training and validation files\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        '/home/jovyan/Symbols/train',\n",
    "        target_size=(45, 45),\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = valid_datagen.flow_from_directory(\n",
    "    '/home/jovyan/Symbols/valid',\n",
    "    shuffle=False,\n",
    "    target_size=(45, 45),\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "zRNEHQ-YurdM"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet152, ResNet50, InceptionV3, DenseNet121, DenseNet201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "cMb4IuhcutKt"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# create the base pre-trained model\n",
    "model.add(ResNet152(input_shape=(45, 45,3), weights='imagenet', include_top=False))\n",
    "# add a global spatial average pooling layer\n",
    "model.add(GlobalAveragePooling2D())\n",
    "# let's add a fully-connected layer\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "# and a logistic layer -- let's say we have 60 classes\n",
    "model.add(Dense(len(train_generator.class_indices), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "LGv69HmSuvR6"
   },
   "outputs": [],
   "source": [
    "# specify training loss function \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.0001),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U3beHCfNuwzh",
    "outputId": "05287c91-7227-4a9c-ee47-3dd8ee7072e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "842/842 [==============================] - ETA: 0s - loss: 0.7001 - accuracy: 0.8240INFO:tensorflow:Assets written to: /home/jovyan/Symbols/Models/resnet152e30/assets\n",
      "842/842 [==============================] - 176s 209ms/step - loss: 0.7001 - accuracy: 0.8240 - val_loss: 178.3596 - val_accuracy: 0.7617 - lr: 1.0000e-04\n",
      "Epoch 2/30\n",
      "842/842 [==============================] - ETA: 0s - loss: 0.2375 - accuracy: 0.9357INFO:tensorflow:Assets written to: /home/jovyan/Symbols/Models/resnet152e30/assets\n",
      "842/842 [==============================] - 168s 199ms/step - loss: 0.2375 - accuracy: 0.9357 - val_loss: 45.5554 - val_accuracy: 0.8843 - lr: 1.0000e-04\n",
      "Epoch 3/30\n",
      "842/842 [==============================] - ETA: 0s - loss: 0.1563 - accuracy: 0.9561INFO:tensorflow:Assets written to: /home/jovyan/Symbols/Models/resnet152e30/assets\n",
      "842/842 [==============================] - 174s 206ms/step - loss: 0.1563 - accuracy: 0.9561 - val_loss: 25.4327 - val_accuracy: 0.8242 - lr: 1.0000e-04\n",
      "Epoch 4/30\n",
      "842/842 [==============================] - ETA: 0s - loss: 0.1236 - accuracy: 0.9655INFO:tensorflow:Assets written to: /home/jovyan/Symbols/Models/resnet152e30/assets\n",
      "842/842 [==============================] - 170s 202ms/step - loss: 0.1236 - accuracy: 0.9655 - val_loss: 6.0164 - val_accuracy: 0.9006 - lr: 1.0000e-04\n",
      "Epoch 5/30\n",
      "842/842 [==============================] - 90s 107ms/step - loss: 0.1137 - accuracy: 0.9709 - val_loss: 14.6381 - val_accuracy: 0.9091 - lr: 1.0000e-04\n",
      "Epoch 6/30\n",
      "842/842 [==============================] - 91s 108ms/step - loss: 0.0977 - accuracy: 0.9735 - val_loss: 19.3339 - val_accuracy: 0.9217 - lr: 1.0000e-04\n",
      "Epoch 7/30\n",
      "842/842 [==============================] - ETA: 0s - loss: 0.0797 - accuracy: 0.9772INFO:tensorflow:Assets written to: /home/jovyan/Symbols/Models/resnet152e30/assets\n",
      "842/842 [==============================] - 174s 206ms/step - loss: 0.0797 - accuracy: 0.9772 - val_loss: 4.0989 - val_accuracy: 0.9382 - lr: 1.0000e-04\n",
      "Epoch 8/30\n",
      "842/842 [==============================] - 90s 107ms/step - loss: 0.0735 - accuracy: 0.9813 - val_loss: 51.0386 - val_accuracy: 0.8954 - lr: 1.0000e-04\n",
      "Epoch 9/30\n",
      "842/842 [==============================] - ETA: 0s - loss: 0.0655 - accuracy: 0.9838INFO:tensorflow:Assets written to: /home/jovyan/Symbols/Models/resnet152e30/assets\n",
      "842/842 [==============================] - 174s 206ms/step - loss: 0.0655 - accuracy: 0.9838 - val_loss: 1.9269 - val_accuracy: 0.9409 - lr: 1.0000e-04\n",
      "Epoch 10/30\n",
      "842/842 [==============================] - 89s 106ms/step - loss: 0.0580 - accuracy: 0.9851 - val_loss: 13.3732 - val_accuracy: 0.8869 - lr: 1.0000e-04\n",
      "Epoch 11/30\n",
      "842/842 [==============================] - 90s 106ms/step - loss: 0.0515 - accuracy: 0.9862 - val_loss: 42.0270 - val_accuracy: 0.9198 - lr: 1.0000e-04\n",
      "Epoch 12/30\n",
      "842/842 [==============================] - 90s 107ms/step - loss: 0.0511 - accuracy: 0.9881 - val_loss: 23.0729 - val_accuracy: 0.9186 - lr: 1.0000e-04\n",
      "Epoch 13/30\n",
      "842/842 [==============================] - 89s 106ms/step - loss: 0.0590 - accuracy: 0.9876 - val_loss: 7.2596 - val_accuracy: 0.9089 - lr: 1.0000e-04\n",
      "Epoch 14/30\n",
      "842/842 [==============================] - 91s 108ms/step - loss: 0.0455 - accuracy: 0.9901 - val_loss: 24.6837 - val_accuracy: 0.9160 - lr: 1.0000e-04\n",
      "Epoch 15/30\n",
      "842/842 [==============================] - 89s 105ms/step - loss: 0.0419 - accuracy: 0.9896 - val_loss: 13.8099 - val_accuracy: 0.9297 - lr: 1.0000e-04\n",
      "Epoch 16/30\n",
      "842/842 [==============================] - 90s 107ms/step - loss: 0.0392 - accuracy: 0.9910 - val_loss: 2.1019 - val_accuracy: 0.9359 - lr: 1.0000e-04\n",
      "Epoch 17/30\n",
      "842/842 [==============================] - ETA: 0s - loss: 0.0379 - accuracy: 0.9911INFO:tensorflow:Assets written to: /home/jovyan/Symbols/Models/resnet152e30/assets\n",
      "842/842 [==============================] - 174s 207ms/step - loss: 0.0379 - accuracy: 0.9911 - val_loss: 1.6123 - val_accuracy: 0.9380 - lr: 1.0000e-04\n",
      "Epoch 18/30\n",
      "842/842 [==============================] - 90s 106ms/step - loss: 0.0284 - accuracy: 0.9928 - val_loss: 15.0029 - val_accuracy: 0.9249 - lr: 1.0000e-04\n",
      "Epoch 19/30\n",
      "842/842 [==============================] - 90s 107ms/step - loss: 0.0296 - accuracy: 0.9922 - val_loss: 8.8799 - val_accuracy: 0.9209 - lr: 1.0000e-04\n",
      "Epoch 20/30\n",
      "842/842 [==============================] - 89s 106ms/step - loss: 0.0297 - accuracy: 0.9927 - val_loss: 6.2715 - val_accuracy: 0.9212 - lr: 1.0000e-04\n",
      "Epoch 21/30\n",
      "842/842 [==============================] - ETA: 0s - loss: 0.0321 - accuracy: 0.9926\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Symbols/Models/resnet152e30/assets\n",
      "842/842 [==============================] - 176s 209ms/step - loss: 0.0321 - accuracy: 0.9926 - val_loss: 0.8565 - val_accuracy: 0.9406 - lr: 1.0000e-04\n",
      "Epoch 22/30\n",
      "842/842 [==============================] - 89s 105ms/step - loss: 0.0135 - accuracy: 0.9970 - val_loss: 4.7197 - val_accuracy: 0.9254 - lr: 5.0000e-05\n",
      "Epoch 23/30\n",
      "842/842 [==============================] - 90s 107ms/step - loss: 0.0055 - accuracy: 0.9986 - val_loss: 9.6293 - val_accuracy: 0.9215 - lr: 5.0000e-05\n",
      "Epoch 24/30\n",
      "842/842 [==============================] - 89s 106ms/step - loss: 0.0060 - accuracy: 0.9985 - val_loss: 5.4570 - val_accuracy: 0.9288 - lr: 5.0000e-05\n",
      "Epoch 25/30\n",
      "842/842 [==============================] - 88s 105ms/step - loss: 0.0048 - accuracy: 0.9984 - val_loss: 4.7301 - val_accuracy: 0.9290 - lr: 5.0000e-05\n",
      "Epoch 26/30\n",
      "842/842 [==============================] - 90s 107ms/step - loss: 0.0065 - accuracy: 0.9990 - val_loss: 4.7564 - val_accuracy: 0.9280 - lr: 5.0000e-05\n",
      "Epoch 27/30\n",
      "842/842 [==============================] - 89s 105ms/step - loss: 0.0031 - accuracy: 0.9992 - val_loss: 12.5062 - val_accuracy: 0.9262 - lr: 5.0000e-05\n",
      "Epoch 28/30\n",
      "842/842 [==============================] - 89s 106ms/step - loss: 0.0039 - accuracy: 0.9991 - val_loss: 7.6025 - val_accuracy: 0.9213 - lr: 5.0000e-05\n",
      "Epoch 29/30\n",
      "842/842 [==============================] - 89s 106ms/step - loss: 0.0045 - accuracy: 0.9990 - val_loss: 8.8028 - val_accuracy: 0.9265 - lr: 5.0000e-05\n",
      "Epoch 30/30\n",
      "842/842 [==============================] - ETA: 0s - loss: 0.0032 - accuracy: 0.9990\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "842/842 [==============================] - 89s 106ms/step - loss: 0.0032 - accuracy: 0.9990 - val_loss: 6.0195 - val_accuracy: 0.9291 - lr: 5.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f312b49b6a0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=3, verbose = 1)\n",
    "# Add the directory in which the model is saved\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='/home/jovyan/Symbols/Models/resnet152e30',\n",
    "    monitor='val_loss',\n",
    "    save_weights_only=False,\n",
    "    save_best_only=True)\n",
    "\n",
    "model.fit(x=train_generator, validation_data=validation_generator, epochs = 30, callbacks=[reduce_lr,model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oEvlVXtYu0r2",
    "outputId": "8ced1d5b-55e0-4f48-daa4-ab69c413cf31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5082 images belonging to 61 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "# Specify test images directory\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    '/home/jovyan/Symbols/test',\n",
    "    shuffle=False,\n",
    "    target_size=(45, 45),\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "dzRCvt1Wu2GR"
   },
   "outputs": [],
   "source": [
    "# test_generator.classes\n",
    "# test_generator.class_indices\n",
    "# test_generator.filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "D9yeC9zau6gg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11097992916174734 0.14167650531286896 0.16843762298307752 0.1887052341597796 0.2081857536402991\n"
     ]
    }
   ],
   "source": [
    "top1 = 0\n",
    "top2 = 0\n",
    "top3 = 0\n",
    "top4 = 0\n",
    "top5 = 0\n",
    "predict = model.predict(test_generator)\n",
    "\n",
    "for i in range(len(predict)):\n",
    "  result = np.argsort(-predict[i])\n",
    "  truth = test_generator.classes[i]\n",
    "  if truth in result[:5]: top5 += 1\n",
    "  if truth in result[:4]: top4 += 1\n",
    "  if truth in result[:3]: top3 += 1\n",
    "  if truth in result[:2]: top2 += 1\n",
    "  if truth in result[:1]: top1 += 1\n",
    "\n",
    "top1 = top1/len(predict)\n",
    "top2 = top2/len(predict)\n",
    "top3 = top3/len(predict)\n",
    "top4 = top4/len(predict)\n",
    "top5 = top5/len(predict)\n",
    "print(top1, top2, top3, top4, top5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Vs-pITaoo8gs"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "model = keras.models.load_model('/home/jovyan/Symbols/Models/resnet152e30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "iBzUCYWEunMl"
   },
   "outputs": [],
   "source": [
    "# Import the appropriate preprocess input for the model\n",
    "# from tensorflow.keras.applications.resnet import preprocess_input\n",
    "# from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "from tensorflow.keras.applications.densenet import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uY5jyCEKupCn",
    "outputId": "c2417a9e-d20b-4491-fc7d-2f8a8bf4493e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26925 images belonging to 60 classes.\n",
      "Found 11540 images belonging to 60 classes.\n"
     ]
    }
   ],
   "source": [
    "# loading training data\n",
    "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
    "                                #    shear_range=0.2,\n",
    "                                  #  zoom_range=0.4,\n",
    "                                  #  rotation_range=30,\n",
    "                                   )\n",
    "\n",
    "valid_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "# Specify the directory of the training and validation files\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        '/home/jovyan/Symbols/train',\n",
    "        target_size=(45, 45),\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = valid_datagen.flow_from_directory(\n",
    "    '/home/jovyan/Symbols/valid',\n",
    "    shuffle=False,\n",
    "    target_size=(45, 45),\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "zRNEHQ-YurdM"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet152, ResNet50, InceptionV3, DenseNet121, DenseNet201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "cMb4IuhcutKt"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# create the base pre-trained model\n",
    "model.add(DenseNet121(input_shape=(45, 45,3), weights='imagenet', include_top=False))\n",
    "# add a global spatial average pooling layer\n",
    "model.add(GlobalAveragePooling2D())\n",
    "# let's add a fully-connected layer\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "# and a logistic layer -- let's say we have 60 classes\n",
    "model.add(Dense(len(train_generator.class_indices), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "LGv69HmSuvR6"
   },
   "outputs": [],
   "source": [
    "# specify training loss function \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.0001),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U3beHCfNuwzh",
    "outputId": "05287c91-7227-4a9c-ee47-3dd8ee7072e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "842/842 [==============================] - ETA: 0s - loss: 0.8038 - accuracy: 0.7969INFO:tensorflow:Assets written to: /home/jovyan/Symbols/Models/densenet121e30/assets\n",
      "842/842 [==============================] - 120s 143ms/step - loss: 0.8038 - accuracy: 0.7969 - val_loss: 0.3955 - val_accuracy: 0.9117 - lr: 1.0000e-04\n",
      "Epoch 2/30\n",
      "842/842 [==============================] - 57s 68ms/step - loss: 0.2411 - accuracy: 0.9297 - val_loss: 0.4510 - val_accuracy: 0.9271 - lr: 1.0000e-04\n",
      "Epoch 3/30\n",
      "842/842 [==============================] - 52s 61ms/step - loss: 0.1713 - accuracy: 0.9498 - val_loss: 0.4408 - val_accuracy: 0.9412 - lr: 1.0000e-04\n",
      "Epoch 4/30\n",
      "842/842 [==============================] - ETA: 0s - loss: 0.1354 - accuracy: 0.9594INFO:tensorflow:Assets written to: /home/jovyan/Symbols/Models/densenet121e30/assets\n",
      "842/842 [==============================] - 113s 134ms/step - loss: 0.1354 - accuracy: 0.9594 - val_loss: 0.3694 - val_accuracy: 0.9464 - lr: 1.0000e-04\n",
      "Epoch 5/30\n",
      "842/842 [==============================] - 58s 69ms/step - loss: 0.1130 - accuracy: 0.9651 - val_loss: 0.3901 - val_accuracy: 0.9409 - lr: 1.0000e-04\n",
      "Epoch 6/30\n",
      "842/842 [==============================] - ETA: 0s - loss: 0.0986 - accuracy: 0.9699INFO:tensorflow:Assets written to: /home/jovyan/Symbols/Models/densenet121e30/assets\n",
      "842/842 [==============================] - 116s 137ms/step - loss: 0.0986 - accuracy: 0.9699 - val_loss: 0.3685 - val_accuracy: 0.9438 - lr: 1.0000e-04\n",
      "Epoch 7/30\n",
      "842/842 [==============================] - 57s 68ms/step - loss: 0.0859 - accuracy: 0.9735 - val_loss: 0.4575 - val_accuracy: 0.9362 - lr: 1.0000e-04\n",
      "Epoch 8/30\n",
      "842/842 [==============================] - 52s 61ms/step - loss: 0.0754 - accuracy: 0.9780 - val_loss: 0.5453 - val_accuracy: 0.9446 - lr: 1.0000e-04\n",
      "Epoch 9/30\n",
      "842/842 [==============================] - 52s 61ms/step - loss: 0.0757 - accuracy: 0.9777 - val_loss: 0.5426 - val_accuracy: 0.9469 - lr: 1.0000e-04\n",
      "Epoch 10/30\n",
      "842/842 [==============================] - 52s 62ms/step - loss: 0.0659 - accuracy: 0.9804 - val_loss: 0.4801 - val_accuracy: 0.9396 - lr: 1.0000e-04\n",
      "Epoch 11/30\n",
      "842/842 [==============================] - 52s 62ms/step - loss: 0.0596 - accuracy: 0.9815 - val_loss: 1.7679 - val_accuracy: 0.9344 - lr: 1.0000e-04\n",
      "Epoch 12/30\n",
      "842/842 [==============================] - 52s 62ms/step - loss: 0.0532 - accuracy: 0.9838 - val_loss: 0.7669 - val_accuracy: 0.9440 - lr: 1.0000e-04\n",
      "Epoch 13/30\n",
      "842/842 [==============================] - 53s 63ms/step - loss: 0.0552 - accuracy: 0.9844 - val_loss: 0.5624 - val_accuracy: 0.9482 - lr: 1.0000e-04\n",
      "Epoch 14/30\n",
      "842/842 [==============================] - 51s 61ms/step - loss: 0.0489 - accuracy: 0.9861 - val_loss: 0.9378 - val_accuracy: 0.9451 - lr: 1.0000e-04\n",
      "Epoch 15/30\n",
      "842/842 [==============================] - 51s 60ms/step - loss: 0.0430 - accuracy: 0.9881 - val_loss: 0.8631 - val_accuracy: 0.9431 - lr: 1.0000e-04\n",
      "Epoch 16/30\n",
      "842/842 [==============================] - 51s 61ms/step - loss: 0.0408 - accuracy: 0.9882 - val_loss: 0.5037 - val_accuracy: 0.9462 - lr: 1.0000e-04\n",
      "Epoch 17/30\n",
      "842/842 [==============================] - 53s 63ms/step - loss: 0.0372 - accuracy: 0.9891 - val_loss: 0.5925 - val_accuracy: 0.9462 - lr: 1.0000e-04\n",
      "Epoch 18/30\n",
      "842/842 [==============================] - 53s 63ms/step - loss: 0.0370 - accuracy: 0.9896 - val_loss: 0.8306 - val_accuracy: 0.9463 - lr: 1.0000e-04\n",
      "Epoch 19/30\n",
      "842/842 [==============================] - 53s 63ms/step - loss: 0.0360 - accuracy: 0.9902 - val_loss: 0.8731 - val_accuracy: 0.9498 - lr: 1.0000e-04\n",
      "Epoch 20/30\n",
      "842/842 [==============================] - 55s 65ms/step - loss: 0.0366 - accuracy: 0.9908 - val_loss: 0.6091 - val_accuracy: 0.9496 - lr: 1.0000e-04\n",
      "Epoch 21/30\n",
      "842/842 [==============================] - 52s 61ms/step - loss: 0.0336 - accuracy: 0.9910 - val_loss: 1.4102 - val_accuracy: 0.9413 - lr: 1.0000e-04\n",
      "Epoch 22/30\n",
      "842/842 [==============================] - 53s 62ms/step - loss: 0.0358 - accuracy: 0.9913 - val_loss: 0.6565 - val_accuracy: 0.9503 - lr: 1.0000e-04\n",
      "Epoch 23/30\n",
      "842/842 [==============================] - 50s 60ms/step - loss: 0.0324 - accuracy: 0.9920 - val_loss: 0.7941 - val_accuracy: 0.9534 - lr: 1.0000e-04\n",
      "Epoch 24/30\n",
      "842/842 [==============================] - 51s 61ms/step - loss: 0.0270 - accuracy: 0.9931 - val_loss: 0.6648 - val_accuracy: 0.9489 - lr: 1.0000e-04\n",
      "Epoch 25/30\n",
      "842/842 [==============================] - 53s 63ms/step - loss: 0.0268 - accuracy: 0.9926 - val_loss: 0.7311 - val_accuracy: 0.9505 - lr: 1.0000e-04\n",
      "Epoch 26/30\n",
      "842/842 [==============================] - 55s 65ms/step - loss: 0.0267 - accuracy: 0.9928 - val_loss: 0.9454 - val_accuracy: 0.9445 - lr: 1.0000e-04\n",
      "Epoch 27/30\n",
      "842/842 [==============================] - 53s 63ms/step - loss: 0.0246 - accuracy: 0.9934 - val_loss: 0.8336 - val_accuracy: 0.9481 - lr: 1.0000e-04\n",
      "Epoch 28/30\n",
      "842/842 [==============================] - 52s 62ms/step - loss: 0.0245 - accuracy: 0.9942 - val_loss: 0.6539 - val_accuracy: 0.9505 - lr: 1.0000e-04\n",
      "Epoch 29/30\n",
      "842/842 [==============================] - 51s 61ms/step - loss: 0.0285 - accuracy: 0.9939 - val_loss: 1.2570 - val_accuracy: 0.9442 - lr: 1.0000e-04\n",
      "Epoch 30/30\n",
      "842/842 [==============================] - 51s 61ms/step - loss: 0.0214 - accuracy: 0.9942 - val_loss: 0.7739 - val_accuracy: 0.9507 - lr: 1.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f312e346748>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=3, verbose = 1)\n",
    "# Specify the directory in which the model is saved\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='/home/jovyan/Symbols/Models/densenet121e30',\n",
    "    monitor='val_loss',\n",
    "    save_weights_only=False,\n",
    "    save_best_only=True)\n",
    "\n",
    "model.fit(x=train_generator, validation_data=validation_generator, epochs = 30, callbacks=[reduce_lr,model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oEvlVXtYu0r2",
    "outputId": "8ced1d5b-55e0-4f48-daa4-ab69c413cf31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5082 images belonging to 61 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "# Specify the test images directory\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    '/home/jovyan/Symbols/test',\n",
    "    shuffle=False,\n",
    "    target_size=(45, 45),\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "dzRCvt1Wu2GR"
   },
   "outputs": [],
   "source": [
    "# test_generator.classes\n",
    "# test_generator.class_indices\n",
    "# test_generator.filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "D9yeC9zau6gg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1127508854781582 0.15348288075560804 0.18850846123573398 0.21251475796930341 0.23415977961432508\n"
     ]
    }
   ],
   "source": [
    "top1 = 0\n",
    "top2 = 0\n",
    "top3 = 0\n",
    "top4 = 0\n",
    "top5 = 0\n",
    "predict = model.predict(test_generator)\n",
    "\n",
    "for i in range(len(predict)):\n",
    "  result = np.argsort(-predict[i])\n",
    "  truth = test_generator.classes[i]\n",
    "  if truth in result[:5]: top5 += 1\n",
    "  if truth in result[:4]: top4 += 1\n",
    "  if truth in result[:3]: top3 += 1\n",
    "  if truth in result[:2]: top2 += 1\n",
    "  if truth in result[:1]: top1 += 1\n",
    "\n",
    "top1 = top1/len(predict)\n",
    "top2 = top2/len(predict)\n",
    "top3 = top3/len(predict)\n",
    "top4 = top4/len(predict)\n",
    "top5 = top5/len(predict)\n",
    "print(top1, top2, top3, top4, top5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Vs-pITaoo8gs"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "model = keras.models.load_model('/home/jovyan/Symbols/Models/densenet121e30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "iBzUCYWEunMl"
   },
   "outputs": [],
   "source": [
    "# Import the corresponding preprocess input\n",
    "\n",
    "# from tensorflow.keras.applications.resnet import preprocess_input\n",
    "# from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "from tensorflow.keras.applications.densenet import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uY5jyCEKupCn",
    "outputId": "c2417a9e-d20b-4491-fc7d-2f8a8bf4493e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26925 images belonging to 60 classes.\n",
      "Found 11540 images belonging to 60 classes.\n"
     ]
    }
   ],
   "source": [
    "# loading training data\n",
    "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
    "                                #    shear_range=0.2,\n",
    "                                  #  zoom_range=0.4,\n",
    "                                  #  rotation_range=30,\n",
    "                                   )\n",
    "\n",
    "valid_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "# Specify the directory of the training and validation files\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        '/home/jovyan/Symbols/train',\n",
    "        target_size=(45, 45),\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = valid_datagen.flow_from_directory(\n",
    "    '/home/jovyan/Symbols/valid',\n",
    "    shuffle=False,\n",
    "    target_size=(45, 45),\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "zRNEHQ-YurdM"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet152, ResNet50, InceptionV3, DenseNet121, DenseNet201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "cMb4IuhcutKt"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# create the base pre-trained model\n",
    "model.add(DenseNet201(input_shape=(45, 45,3), weights='imagenet', include_top=False))\n",
    "# add a global spatial average pooling layer\n",
    "model.add(GlobalAveragePooling2D())\n",
    "# let's add a fully-connected layer\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "# and a logistic layer -- let's say we have 60 classes\n",
    "model.add(Dense(len(train_generator.class_indices), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "LGv69HmSuvR6"
   },
   "outputs": [],
   "source": [
    "# specify training loss function \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.0001),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U3beHCfNuwzh",
    "outputId": "05287c91-7227-4a9c-ee47-3dd8ee7072e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "842/842 [==============================] - ETA: 0s - loss: 0.6836 - accuracy: 0.8303INFO:tensorflow:Assets written to: /home/jovyan/Symbols/Models/densenet201e30/assets\n",
      "842/842 [==============================] - 193s 229ms/step - loss: 0.6836 - accuracy: 0.8303 - val_loss: 0.4583 - val_accuracy: 0.9166 - lr: 1.0000e-04\n",
      "Epoch 2/30\n",
      "842/842 [==============================] - 88s 105ms/step - loss: 0.2231 - accuracy: 0.9380 - val_loss: 0.6642 - val_accuracy: 0.9336 - lr: 1.0000e-04\n",
      "Epoch 3/30\n",
      "842/842 [==============================] - ETA: 0s - loss: 0.1661 - accuracy: 0.9511INFO:tensorflow:Assets written to: /home/jovyan/Symbols/Models/densenet201e30/assets\n",
      "842/842 [==============================] - 192s 228ms/step - loss: 0.1661 - accuracy: 0.9511 - val_loss: 0.4288 - val_accuracy: 0.9365 - lr: 1.0000e-04\n",
      "Epoch 4/30\n",
      "842/842 [==============================] - 90s 107ms/step - loss: 0.1354 - accuracy: 0.9618 - val_loss: 0.7661 - val_accuracy: 0.9365 - lr: 1.0000e-04\n",
      "Epoch 5/30\n",
      "842/842 [==============================] - ETA: 0s - loss: 0.1157 - accuracy: 0.9666INFO:tensorflow:Assets written to: /home/jovyan/Symbols/Models/densenet201e30/assets\n",
      "842/842 [==============================] - 190s 225ms/step - loss: 0.1157 - accuracy: 0.9666 - val_loss: 0.3823 - val_accuracy: 0.9477 - lr: 1.0000e-04\n",
      "Epoch 6/30\n",
      "842/842 [==============================] - 90s 106ms/step - loss: 0.1004 - accuracy: 0.9711 - val_loss: 0.4532 - val_accuracy: 0.9500 - lr: 1.0000e-04\n",
      "Epoch 7/30\n",
      "842/842 [==============================] - 93s 110ms/step - loss: 0.0904 - accuracy: 0.9732 - val_loss: 0.5740 - val_accuracy: 0.9447 - lr: 1.0000e-04\n",
      "Epoch 8/30\n",
      "842/842 [==============================] - 90s 107ms/step - loss: 0.0809 - accuracy: 0.9763 - val_loss: 0.4199 - val_accuracy: 0.9423 - lr: 1.0000e-04\n",
      "Epoch 9/30\n",
      "842/842 [==============================] - 93s 110ms/step - loss: 0.0673 - accuracy: 0.9798 - val_loss: 0.7068 - val_accuracy: 0.9425 - lr: 1.0000e-04\n",
      "Epoch 10/30\n",
      "842/842 [==============================] - 91s 108ms/step - loss: 0.0656 - accuracy: 0.9814 - val_loss: 0.5703 - val_accuracy: 0.9496 - lr: 1.0000e-04\n",
      "Epoch 11/30\n",
      "842/842 [==============================] - 92s 109ms/step - loss: 0.0660 - accuracy: 0.9823 - val_loss: 0.9273 - val_accuracy: 0.9409 - lr: 1.0000e-04\n",
      "Epoch 12/30\n",
      "842/842 [==============================] - 93s 110ms/step - loss: 0.0626 - accuracy: 0.9840 - val_loss: 0.6515 - val_accuracy: 0.9466 - lr: 1.0000e-04\n",
      "Epoch 13/30\n",
      "842/842 [==============================] - 90s 107ms/step - loss: 0.0622 - accuracy: 0.9839 - val_loss: 0.6385 - val_accuracy: 0.9475 - lr: 1.0000e-04\n",
      "Epoch 14/30\n",
      "842/842 [==============================] - 93s 111ms/step - loss: 0.0549 - accuracy: 0.9858 - val_loss: 0.8661 - val_accuracy: 0.9469 - lr: 1.0000e-04\n",
      "Epoch 15/30\n",
      "842/842 [==============================] - 92s 109ms/step - loss: 0.0480 - accuracy: 0.9874 - val_loss: 1.3336 - val_accuracy: 0.9419 - lr: 1.0000e-04\n",
      "Epoch 16/30\n",
      "842/842 [==============================] - 92s 110ms/step - loss: 0.0469 - accuracy: 0.9876 - val_loss: 0.6907 - val_accuracy: 0.9510 - lr: 1.0000e-04\n",
      "Epoch 17/30\n",
      "842/842 [==============================] - 92s 110ms/step - loss: 0.0459 - accuracy: 0.9886 - val_loss: 0.9107 - val_accuracy: 0.9422 - lr: 1.0000e-04\n",
      "Epoch 18/30\n",
      "842/842 [==============================] - 90s 107ms/step - loss: 0.0467 - accuracy: 0.9888 - val_loss: 0.6632 - val_accuracy: 0.9519 - lr: 1.0000e-04\n",
      "Epoch 19/30\n",
      "842/842 [==============================] - 93s 110ms/step - loss: 0.0392 - accuracy: 0.9890 - val_loss: 0.5820 - val_accuracy: 0.9516 - lr: 1.0000e-04\n",
      "Epoch 20/30\n",
      "842/842 [==============================] - 90s 107ms/step - loss: 0.0375 - accuracy: 0.9908 - val_loss: 0.9403 - val_accuracy: 0.9413 - lr: 1.0000e-04\n",
      "Epoch 21/30\n",
      "842/842 [==============================] - 93s 110ms/step - loss: 0.0388 - accuracy: 0.9900 - val_loss: 0.8017 - val_accuracy: 0.9460 - lr: 1.0000e-04\n",
      "Epoch 22/30\n",
      "842/842 [==============================] - 92s 110ms/step - loss: 0.0349 - accuracy: 0.9918 - val_loss: 0.9497 - val_accuracy: 0.9402 - lr: 1.0000e-04\n",
      "Epoch 23/30\n",
      "842/842 [==============================] - 92s 109ms/step - loss: 0.0363 - accuracy: 0.9916 - val_loss: 2.6517 - val_accuracy: 0.9286 - lr: 1.0000e-04\n",
      "Epoch 24/30\n",
      "842/842 [==============================] - 93s 110ms/step - loss: 0.0384 - accuracy: 0.9917 - val_loss: 0.8698 - val_accuracy: 0.9473 - lr: 1.0000e-04\n",
      "Epoch 25/30\n",
      "842/842 [==============================] - 90s 107ms/step - loss: 0.0342 - accuracy: 0.9925 - val_loss: 0.9897 - val_accuracy: 0.9463 - lr: 1.0000e-04\n",
      "Epoch 26/30\n",
      "842/842 [==============================] - 92s 109ms/step - loss: 0.0310 - accuracy: 0.9923 - val_loss: 0.9312 - val_accuracy: 0.9514 - lr: 1.0000e-04\n",
      "Epoch 27/30\n",
      "842/842 [==============================] - 90s 107ms/step - loss: 0.0303 - accuracy: 0.9932 - val_loss: 1.3247 - val_accuracy: 0.9423 - lr: 1.0000e-04\n",
      "Epoch 28/30\n",
      "842/842 [==============================] - 92s 110ms/step - loss: 0.0328 - accuracy: 0.9928 - val_loss: 7.2769 - val_accuracy: 0.9245 - lr: 1.0000e-04\n",
      "Epoch 29/30\n",
      "842/842 [==============================] - 90s 107ms/step - loss: 0.0325 - accuracy: 0.9931 - val_loss: 1.0980 - val_accuracy: 0.9486 - lr: 1.0000e-04\n",
      "Epoch 30/30\n",
      "842/842 [==============================] - 92s 109ms/step - loss: 0.0279 - accuracy: 0.9936 - val_loss: 0.8142 - val_accuracy: 0.9494 - lr: 1.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3105df00b8>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=3, verbose = 1)\n",
    "# Specify the filepath for the model to be saved\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='/home/jovyan/Symbols/Models/densenet201e30',\n",
    "    monitor='val_loss',\n",
    "    save_weights_only=False,\n",
    "    save_best_only=True)\n",
    "\n",
    "model.fit(x=train_generator, validation_data=validation_generator, epochs = 30, callbacks=[reduce_lr,model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oEvlVXtYu0r2",
    "outputId": "8ced1d5b-55e0-4f48-daa4-ab69c413cf31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5082 images belonging to 61 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    '/home/jovyan/Symbols/test',\n",
    "    shuffle=False,\n",
    "    target_size=(45, 45),\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "dzRCvt1Wu2GR"
   },
   "outputs": [],
   "source": [
    "# test_generator.classes\n",
    "# test_generator.class_indices\n",
    "# test_generator.filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "D9yeC9zau6gg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11078315623770169 0.15387642660369932 0.170995670995671 0.18752459661550572 0.20031483667847305\n"
     ]
    }
   ],
   "source": [
    "top1 = 0\n",
    "top2 = 0\n",
    "top3 = 0\n",
    "top4 = 0\n",
    "top5 = 0\n",
    "predict = model.predict(test_generator)\n",
    "\n",
    "for i in range(len(predict)):\n",
    "  result = np.argsort(-predict[i])\n",
    "  truth = test_generator.classes[i]\n",
    "  if truth in result[:5]: top5 += 1\n",
    "  if truth in result[:4]: top4 += 1\n",
    "  if truth in result[:3]: top3 += 1\n",
    "  if truth in result[:2]: top2 += 1\n",
    "  if truth in result[:1]: top1 += 1\n",
    "\n",
    "top1 = top1/len(predict)\n",
    "top2 = top2/len(predict)\n",
    "top3 = top3/len(predict)\n",
    "top4 = top4/len(predict)\n",
    "top5 = top5/len(predict)\n",
    "print(top1, top2, top3, top4, top5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Vs-pITaoo8gs"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "model = keras.models.load_model('/home/jovyan/Symbols/Models/densenet201e30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
