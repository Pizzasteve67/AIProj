{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o1pZN-2oyg8S",
    "outputId": "b22655b5-8214-45ae-907f-d7ebbdb61fa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "TsKx_kzknLX6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.python.keras.layers import Dense, Flatten, GlobalAveragePooling2D\n",
    "from keras import optimizers\n",
    "from keras import applications\n",
    "from keras.models import Model\n",
    "from keras.callbacks import History\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "iBzUCYWEunMl"
   },
   "outputs": [],
   "source": [
    "# Import the corresponding preprocess inputs\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "# from tensorflow.keras.applications.vgg19 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uY5jyCEKupCn",
    "outputId": "c2417a9e-d20b-4491-fc7d-2f8a8bf4493e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26925 images belonging to 60 classes.\n",
      "Found 11540 images belonging to 60 classes.\n"
     ]
    }
   ],
   "source": [
    "# loading training data\n",
    "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input\n",
    "                                   )\n",
    "\n",
    "valid_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "# Specify the directory of the training and validation files\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        '/home/jovyan/Symbols/train',\n",
    "        target_size=(45, 45),\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = valid_datagen.flow_from_directory(\n",
    "    '/home/jovyan/Symbols/valid',\n",
    "    shuffle=False,\n",
    "    target_size=(45, 45),\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "zRNEHQ-YurdM"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "cMb4IuhcutKt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# create the base pre-trained model\n",
    "model.add(VGG16(input_shape=(75, 75,3), weights='imagenet', include_top=False))\n",
    "# add a global spatial average pooling layer\n",
    "model.add(GlobalAveragePooling2D())\n",
    "# let's add a fully-connected layer\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "# and a logistic layer -- let's say we have 60 classes\n",
    "model.add(Dense(len(train_generator.class_indices), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "LGv69HmSuvR6"
   },
   "outputs": [],
   "source": [
    "# specify training loss function \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.0001),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U3beHCfNuwzh",
    "outputId": "05287c91-7227-4a9c-ee47-3dd8ee7072e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "842/842 [==============================] - ETA: 0s - loss: 0.9544 - accuracy: 0.7635WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/Symbols/Models/vgg16e30/assets\n",
      "842/842 [==============================] - 51s 60ms/step - loss: 0.9544 - accuracy: 0.7635 - val_loss: 0.4110 - val_accuracy: 0.8926 - lr: 1.0000e-04\n",
      "Epoch 2/30\n",
      "842/842 [==============================] - 23s 27ms/step - loss: 0.2575 - accuracy: 0.9340 - val_loss: 0.5757 - val_accuracy: 0.9268 - lr: 1.0000e-04\n",
      "Epoch 3/30\n",
      "842/842 [==============================] - 18s 22ms/step - loss: 0.2015 - accuracy: 0.9487 - val_loss: 0.4272 - val_accuracy: 0.9298 - lr: 1.0000e-04\n",
      "Epoch 4/30\n",
      "842/842 [==============================] - 18s 22ms/step - loss: 0.2032 - accuracy: 0.9525 - val_loss: 0.7026 - val_accuracy: 0.9172 - lr: 1.0000e-04\n",
      "Epoch 5/30\n",
      "842/842 [==============================] - 18s 21ms/step - loss: 0.2126 - accuracy: 0.9510 - val_loss: 1.0561 - val_accuracy: 0.9269 - lr: 1.0000e-04\n",
      "Epoch 6/30\n",
      "841/842 [============================>.] - ETA: 0s - loss: 0.2191 - accuracy: 0.9492\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "842/842 [==============================] - 19s 22ms/step - loss: 0.2193 - accuracy: 0.9492 - val_loss: 0.5814 - val_accuracy: 0.9224 - lr: 1.0000e-04\n",
      "Epoch 7/30\n",
      "842/842 [==============================] - 18s 22ms/step - loss: 0.1915 - accuracy: 0.9690 - val_loss: 0.5304 - val_accuracy: 0.9361 - lr: 5.0000e-05\n",
      "Epoch 8/30\n",
      "842/842 [==============================] - 19s 23ms/step - loss: 0.1552 - accuracy: 0.9693 - val_loss: 1.6282 - val_accuracy: 0.9322 - lr: 5.0000e-05\n",
      "Epoch 9/30\n",
      "842/842 [==============================] - 19s 22ms/step - loss: 0.1522 - accuracy: 0.9665 - val_loss: 0.8127 - val_accuracy: 0.9383 - lr: 5.0000e-05\n",
      "Epoch 10/30\n",
      "842/842 [==============================] - 18s 22ms/step - loss: 0.1561 - accuracy: 0.9669 - val_loss: 2.0265 - val_accuracy: 0.9250 - lr: 5.0000e-05\n",
      "Epoch 11/30\n",
      "842/842 [==============================] - 18s 22ms/step - loss: 0.2197 - accuracy: 0.9616 - val_loss: 0.9069 - val_accuracy: 0.9289 - lr: 5.0000e-05\n",
      "Epoch 12/30\n",
      "840/842 [============================>.] - ETA: 0s - loss: 1.0631 - accuracy: 0.9621\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "842/842 [==============================] - 18s 22ms/step - loss: 1.0609 - accuracy: 0.9622 - val_loss: 1.0915 - val_accuracy: 0.9212 - lr: 5.0000e-05\n",
      "Epoch 13/30\n",
      "842/842 [==============================] - 18s 22ms/step - loss: 0.0820 - accuracy: 0.9779 - val_loss: 2.6865 - val_accuracy: 0.9304 - lr: 2.5000e-05\n",
      "Epoch 14/30\n",
      "842/842 [==============================] - 18s 22ms/step - loss: 0.0976 - accuracy: 0.9784 - val_loss: 3.2365 - val_accuracy: 0.9436 - lr: 2.5000e-05\n",
      "Epoch 15/30\n",
      "842/842 [==============================] - 18s 22ms/step - loss: 0.1446 - accuracy: 0.9782 - val_loss: 3.7659 - val_accuracy: 0.9386 - lr: 2.5000e-05\n",
      "Epoch 16/30\n",
      "842/842 [==============================] - ETA: 0s - loss: 0.0970 - accuracy: 0.9776\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "842/842 [==============================] - 18s 22ms/step - loss: 0.0970 - accuracy: 0.9776 - val_loss: 2.5372 - val_accuracy: 0.9471 - lr: 2.5000e-05\n",
      "Epoch 17/30\n",
      "842/842 [==============================] - 18s 22ms/step - loss: 0.1956 - accuracy: 0.9841 - val_loss: 7.3692 - val_accuracy: 0.9458 - lr: 1.2500e-05\n",
      "Epoch 18/30\n",
      "842/842 [==============================] - 18s 22ms/step - loss: 0.0568 - accuracy: 0.9856 - val_loss: 33.0535 - val_accuracy: 0.9406 - lr: 1.2500e-05\n",
      "Epoch 19/30\n",
      "842/842 [==============================] - 18s 22ms/step - loss: 0.5455 - accuracy: 0.9843 - val_loss: 11.9389 - val_accuracy: 0.9451 - lr: 1.2500e-05\n",
      "Epoch 20/30\n",
      "842/842 [==============================] - 18s 22ms/step - loss: 0.0793 - accuracy: 0.9849 - val_loss: 10.7358 - val_accuracy: 0.9476 - lr: 1.2500e-05\n",
      "Epoch 21/30\n",
      "841/842 [============================>.] - ETA: 0s - loss: 0.0659 - accuracy: 0.9844\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "842/842 [==============================] - 18s 22ms/step - loss: 0.0658 - accuracy: 0.9844 - val_loss: 9.7965 - val_accuracy: 0.9403 - lr: 1.2500e-05\n",
      "Epoch 22/30\n",
      "842/842 [==============================] - 18s 22ms/step - loss: 0.0875 - accuracy: 0.9884 - val_loss: 20.1820 - val_accuracy: 0.9482 - lr: 6.2500e-06\n",
      "Epoch 23/30\n",
      "842/842 [==============================] - 18s 22ms/step - loss: 0.0824 - accuracy: 0.9896 - val_loss: 27.2530 - val_accuracy: 0.9465 - lr: 6.2500e-06\n",
      "Epoch 24/30\n",
      "840/842 [============================>.] - ETA: 0s - loss: 0.0666 - accuracy: 0.9894\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "842/842 [==============================] - 18s 21ms/step - loss: 0.0665 - accuracy: 0.9893 - val_loss: 24.5521 - val_accuracy: 0.9488 - lr: 6.2500e-06\n",
      "Epoch 25/30\n",
      "842/842 [==============================] - 18s 22ms/step - loss: 0.0485 - accuracy: 0.9916 - val_loss: 32.1158 - val_accuracy: 0.9494 - lr: 3.1250e-06\n",
      "Epoch 26/30\n",
      "842/842 [==============================] - 18s 22ms/step - loss: 0.0404 - accuracy: 0.9918 - val_loss: 36.6487 - val_accuracy: 0.9491 - lr: 3.1250e-06\n",
      "Epoch 27/30\n",
      "842/842 [==============================] - 18s 22ms/step - loss: 0.0385 - accuracy: 0.9924 - val_loss: 40.7416 - val_accuracy: 0.9477 - lr: 3.1250e-06\n",
      "Epoch 28/30\n",
      "842/842 [==============================] - 18s 22ms/step - loss: 0.0285 - accuracy: 0.9929 - val_loss: 52.3106 - val_accuracy: 0.9474 - lr: 3.1250e-06\n",
      "Epoch 29/30\n",
      "842/842 [==============================] - 19s 22ms/step - loss: 0.0266 - accuracy: 0.9926 - val_loss: 59.9063 - val_accuracy: 0.9442 - lr: 3.1250e-06\n",
      "Epoch 30/30\n",
      "842/842 [==============================] - 18s 22ms/step - loss: 0.0665 - accuracy: 0.9931 - val_loss: 49.7710 - val_accuracy: 0.9477 - lr: 3.1250e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f91643c10b8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=3, verbose = 1)\n",
    "\n",
    "# Specify the model path\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='/home/jovyan/Symbols/Models/vgg16e30',\n",
    "    monitor='val_loss',\n",
    "    save_weights_only=False,\n",
    "    save_best_only=True)\n",
    "\n",
    "model.fit(x=train_generator, validation_data=validation_generator, epochs = 30, callbacks=[reduce_lr,model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Vs-pITaoo8gs"
   },
   "outputs": [],
   "source": [
    "# model = keras.models.load_model('/home/jovyan/Symbols/Models/InceptionV3e30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "iBzUCYWEunMl"
   },
   "outputs": [],
   "source": [
    "# Import the appropriate preprocess input\n",
    "# from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uY5jyCEKupCn",
    "outputId": "c2417a9e-d20b-4491-fc7d-2f8a8bf4493e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26925 images belonging to 60 classes.\n",
      "Found 11540 images belonging to 60 classes.\n"
     ]
    }
   ],
   "source": [
    "# loading training data\n",
    "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input\n",
    "                                   )\n",
    "\n",
    "valid_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "# Specify the directory of the training and validation files\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        '/home/jovyan/Symbols/train',\n",
    "        target_size=(45, 45),\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = valid_datagen.flow_from_directory(\n",
    "    '/home/jovyan/Symbols/valid',\n",
    "    shuffle=False,\n",
    "    target_size=(45, 45),\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "zRNEHQ-YurdM"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "cMb4IuhcutKt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "80142336/80134624 [==============================] - 4s 0us/step\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# create the base pre-trained model\n",
    "model.add(VGG19(input_shape=(45, 45,3), weights='imagenet', include_top=False))\n",
    "# add a global spatial average pooling layer\n",
    "model.add(GlobalAveragePooling2D())\n",
    "# let's add a fully-connected layer\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "# and a logistic layer -- let's say we have 60 classes\n",
    "model.add(Dense(len(train_generator.class_indices), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "LGv69HmSuvR6"
   },
   "outputs": [],
   "source": [
    "# specify training loss function \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.0001),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U3beHCfNuwzh",
    "outputId": "05287c91-7227-4a9c-ee47-3dd8ee7072e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "841/842 [============================>.] - ETA: 0s - loss: 1.1056 - accuracy: 0.7279INFO:tensorflow:Assets written to: /home/jovyan/Symbols/Models/vgg19e30/assets\n",
      "842/842 [==============================] - 24s 28ms/step - loss: 1.1047 - accuracy: 0.7280 - val_loss: 0.5474 - val_accuracy: 0.8890 - lr: 1.0000e-04\n",
      "Epoch 2/30\n",
      "842/842 [==============================] - 21s 26ms/step - loss: 0.2851 - accuracy: 0.9278 - val_loss: 0.5522 - val_accuracy: 0.8789 - lr: 1.0000e-04\n",
      "Epoch 3/30\n",
      "842/842 [==============================] - ETA: 0s - loss: 0.2087 - accuracy: 0.9457INFO:tensorflow:Assets written to: /home/jovyan/Symbols/Models/vgg19e30/assets\n",
      "842/842 [==============================] - 24s 28ms/step - loss: 0.2087 - accuracy: 0.9457 - val_loss: 0.4366 - val_accuracy: 0.9276 - lr: 1.0000e-04\n",
      "Epoch 4/30\n",
      "841/842 [============================>.] - ETA: 0s - loss: 0.1948 - accuracy: 0.9514INFO:tensorflow:Assets written to: /home/jovyan/Symbols/Models/vgg19e30/assets\n",
      "842/842 [==============================] - 24s 29ms/step - loss: 0.1947 - accuracy: 0.9514 - val_loss: 0.4364 - val_accuracy: 0.9326 - lr: 1.0000e-04\n",
      "Epoch 5/30\n",
      "840/842 [============================>.] - ETA: 0s - loss: 0.1846 - accuracy: 0.9531INFO:tensorflow:Assets written to: /home/jovyan/Symbols/Models/vgg19e30/assets\n",
      "842/842 [==============================] - 24s 28ms/step - loss: 0.1845 - accuracy: 0.9531 - val_loss: 0.3591 - val_accuracy: 0.9255 - lr: 1.0000e-04\n",
      "Epoch 6/30\n",
      "842/842 [==============================] - 21s 25ms/step - loss: 0.2113 - accuracy: 0.9575 - val_loss: 0.5107 - val_accuracy: 0.9194 - lr: 1.0000e-04\n",
      "Epoch 7/30\n",
      "842/842 [==============================] - 21s 25ms/step - loss: 0.2118 - accuracy: 0.9562 - val_loss: 1.1553 - val_accuracy: 0.9373 - lr: 1.0000e-04\n",
      "Epoch 8/30\n",
      "842/842 [==============================] - ETA: 0s - loss: 0.2223 - accuracy: 0.9518\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "842/842 [==============================] - 21s 25ms/step - loss: 0.2223 - accuracy: 0.9518 - val_loss: 1.5368 - val_accuracy: 0.9043 - lr: 1.0000e-04\n",
      "Epoch 9/30\n",
      "842/842 [==============================] - 21s 25ms/step - loss: 0.2378 - accuracy: 0.9697 - val_loss: 0.8216 - val_accuracy: 0.9451 - lr: 5.0000e-05\n",
      "Epoch 10/30\n",
      "842/842 [==============================] - 21s 25ms/step - loss: 0.1324 - accuracy: 0.9705 - val_loss: 3.2130 - val_accuracy: 0.9412 - lr: 5.0000e-05\n",
      "Epoch 11/30\n",
      "842/842 [==============================] - 21s 25ms/step - loss: 0.1455 - accuracy: 0.9682 - val_loss: 0.6337 - val_accuracy: 0.9412 - lr: 5.0000e-05\n",
      "Epoch 12/30\n",
      "842/842 [==============================] - 21s 25ms/step - loss: 0.1790 - accuracy: 0.9663 - val_loss: 1.1869 - val_accuracy: 0.9381 - lr: 5.0000e-05\n",
      "Epoch 13/30\n",
      "840/842 [============================>.] - ETA: 0s - loss: 0.1788 - accuracy: 0.9642\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "842/842 [==============================] - 21s 25ms/step - loss: 0.1787 - accuracy: 0.9642 - val_loss: 1.4850 - val_accuracy: 0.9306 - lr: 5.0000e-05\n",
      "Epoch 14/30\n",
      "842/842 [==============================] - 21s 25ms/step - loss: 0.0899 - accuracy: 0.9782 - val_loss: 4.9411 - val_accuracy: 0.9464 - lr: 2.5000e-05\n",
      "Epoch 15/30\n",
      "842/842 [==============================] - 21s 25ms/step - loss: 0.1200 - accuracy: 0.9782 - val_loss: 7.2072 - val_accuracy: 0.9460 - lr: 2.5000e-05\n",
      "Epoch 16/30\n",
      "842/842 [==============================] - 21s 25ms/step - loss: 0.1224 - accuracy: 0.9777 - val_loss: 6.0980 - val_accuracy: 0.9392 - lr: 2.5000e-05\n",
      "Epoch 17/30\n",
      "840/842 [============================>.] - ETA: 0s - loss: 0.1486 - accuracy: 0.9769\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "842/842 [==============================] - 21s 25ms/step - loss: 0.1488 - accuracy: 0.9768 - val_loss: 5.3152 - val_accuracy: 0.9406 - lr: 2.5000e-05\n",
      "Epoch 18/30\n",
      "842/842 [==============================] - 21s 25ms/step - loss: 0.0888 - accuracy: 0.9838 - val_loss: 5.6909 - val_accuracy: 0.9448 - lr: 1.2500e-05\n",
      "Epoch 19/30\n",
      "842/842 [==============================] - 21s 25ms/step - loss: 0.0878 - accuracy: 0.9862 - val_loss: 8.0611 - val_accuracy: 0.9427 - lr: 1.2500e-05\n",
      "Epoch 20/30\n",
      "842/842 [==============================] - 21s 25ms/step - loss: 0.1164 - accuracy: 0.9856 - val_loss: 9.1036 - val_accuracy: 0.9422 - lr: 1.2500e-05\n",
      "Epoch 21/30\n",
      "842/842 [==============================] - 21s 25ms/step - loss: 0.1382 - accuracy: 0.9851 - val_loss: 17.2153 - val_accuracy: 0.9466 - lr: 1.2500e-05\n",
      "Epoch 22/30\n",
      "841/842 [============================>.] - ETA: 0s - loss: 0.1381 - accuracy: 0.9852\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "842/842 [==============================] - 21s 25ms/step - loss: 0.1380 - accuracy: 0.9852 - val_loss: 9.1109 - val_accuracy: 0.9437 - lr: 1.2500e-05\n",
      "Epoch 23/30\n",
      "842/842 [==============================] - 21s 25ms/step - loss: 0.0772 - accuracy: 0.9893 - val_loss: 12.0218 - val_accuracy: 0.9468 - lr: 6.2500e-06\n",
      "Epoch 24/30\n",
      "842/842 [==============================] - 21s 25ms/step - loss: 0.0472 - accuracy: 0.9898 - val_loss: 20.6251 - val_accuracy: 0.9477 - lr: 6.2500e-06\n",
      "Epoch 25/30\n",
      "842/842 [==============================] - 21s 25ms/step - loss: 0.0571 - accuracy: 0.9906 - val_loss: 20.2969 - val_accuracy: 0.9488 - lr: 6.2500e-06\n",
      "Epoch 26/30\n",
      "842/842 [==============================] - 21s 25ms/step - loss: 0.0611 - accuracy: 0.9908 - val_loss: 31.8880 - val_accuracy: 0.9485 - lr: 6.2500e-06\n",
      "Epoch 27/30\n",
      "840/842 [============================>.] - ETA: 0s - loss: 0.1013 - accuracy: 0.9907\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "842/842 [==============================] - 21s 25ms/step - loss: 0.1012 - accuracy: 0.9907 - val_loss: 31.1227 - val_accuracy: 0.9464 - lr: 6.2500e-06\n",
      "Epoch 28/30\n",
      "842/842 [==============================] - 21s 25ms/step - loss: 0.0432 - accuracy: 0.9935 - val_loss: 45.4783 - val_accuracy: 0.9490 - lr: 3.1250e-06\n",
      "Epoch 29/30\n",
      "842/842 [==============================] - 21s 25ms/step - loss: 0.0741 - accuracy: 0.9939 - val_loss: 42.0093 - val_accuracy: 0.9502 - lr: 3.1250e-06\n",
      "Epoch 30/30\n",
      "842/842 [==============================] - 21s 25ms/step - loss: 0.0681 - accuracy: 0.9942 - val_loss: 34.9296 - val_accuracy: 0.9464 - lr: 3.1250e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f91640cffd0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=3, verbose = 1)\n",
    "\n",
    "# Specify the model path\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='/home/jovyan/Symbols/Models/vgg19e30',\n",
    "    monitor='val_loss',\n",
    "    save_weights_only=False,\n",
    "    save_best_only=True)\n",
    "\n",
    "model.fit(x=train_generator, validation_data=validation_generator, epochs = 30, callbacks=[reduce_lr,model_checkpoint])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
